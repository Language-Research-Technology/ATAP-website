<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=robots content="noodp">
<meta http-equiv=x-ua-compatible content="IE=edge, chrome=1">
<title>Useful Methods - ATAP - Australian Text Analytics Platform</title><meta name=Description content="This is My New Hugo Site"><meta property="og:title" content="Useful Methods">
<meta property="og:description" content="Introduction Throughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory LADAL at the University of Queensland. We also have given references to published research using the methods we discuss.
LADAL has an overview of text analysis and distant reading. (context?) Counting Words More complex methods – classification More complex methods – others Visualisation Counting words When we count words, what are we trying to discover?">
<meta property="og:type" content="article">
<meta property="og:url" content="https://language-research-technology.github.io/ATAP-website/methods/"><meta property="og:image" content="https://language-research-technology.github.io/logo.png"><meta property="article:section" content>
<meta property="article:published_time" content="2022-02-15T16:46:42+10:00">
<meta property="article:modified_time" content="2022-02-15T16:46:42+10:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://language-research-technology.github.io/logo.png">
<meta name=twitter:title content="Useful Methods">
<meta name=twitter:description content="Introduction Throughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory LADAL at the University of Queensland. We also have given references to published research using the methods we discuss.
LADAL has an overview of text analysis and distant reading. (context?) Counting Words More complex methods – classification More complex methods – others Visualisation Counting words When we count words, what are we trying to discover?">
<meta name=application-name content="ATAP - Australian Text Analytics Platform">
<meta name=apple-mobile-web-app-title content="ATAP - Australian Text Analytics Platform"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://language-research-technology.github.io/ATAP-website/methods/><link rel=next href=https://language-research-technology.github.io/ATAP-website/data_prep/><link rel=stylesheet href=/ATAP-website/lib/normalize/normalize.min.css><link rel=stylesheet href=/ATAP-website/css/style.min.css><link rel=stylesheet href=/ATAP-website/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/ATAP-website/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Useful Methods","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/language-research-technology.github.io\/ATAP-website\/methods\/"},"genre":"page","wordcount":2759,"url":"https:\/\/language-research-technology.github.io\/ATAP-website\/methods\/","datePublished":"2022-02-15T16:46:42+10:00","dateModified":"2022-02-15T16:46:42+10:00","publisher":{"@type":"Organization","name":"Australian Text Analytics Platform"},"author":{"@type":"Person","name":"Australian Text Analytics Platform"},"description":""}</script></head><body header-desktop header-mobile><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script>
<div id=mask></div><div class=wrapper><header class=desktop id=header-desktop>
<div class=header-wrapper>
<div class=header-title>
<img src=https://language-research-technology.github.io/ATAP-website//ATAP_logo-sm.png alt="ATAP - Australian Text Analytics Platform logo">
</div><div class=menu>
<div class=menu-inner><a class=menu-item href=/ATAP-website/> Home </a><a class=menu-item href=/ATAP-website/posts/> Blog </a><a class=menu-item href=/ATAP-website/text_analysis/> Text Analysis </a><a class=menu-item href=/ATAP-website/resources/> Resources </a><a class=menu-item href=/ATAP-website/organisation/> Organisation </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-desktop>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
</a>
</div></div></div></header><header class=mobile id=header-mobile>
<div class=header-container>
<div class=header-wrapper>
<div class=header-title>
<a href=/ATAP-website/ title="ATAP - Australian Text Analytics Platform">ATAP - Australian Text Analytics Platform</a>
</div><div class=menu-toggle id=menu-toggle-mobile>
<span></span><span></span><span></span>
</div></div><div class=menu id=menu-mobile><div class=search-wrapper>
<div class="search mobile" id=search-mobile>
<input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-mobile>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>
Cancel
</a>
</div><a class=menu-item href=/ATAP-website/ title>Home</a><a class=menu-item href=/ATAP-website/posts/ title>Blog</a><a class=menu-item href=/ATAP-website/text_analysis/ title>Text Analysis</a><a class=menu-item href=/ATAP-website/resources/ title>Resources</a><a class=menu-item href=/ATAP-website/organisation/ title>Organisation</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
</a></div></div></header><div class="search-dropdown desktop">
<div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile">
<div id=search-dropdown-mobile></div></div><main class=main>
<div class=container><div class="page single special"><h1 class="single-title animated pulse faster custom">Useful Methods</h1><br>
<br><div class="content custom" id=content><h3 id=introduction>Introduction</h3><p>Throughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory <a href=https://slcladal.github.io/ target=_blank rel="noopener noreffer">LADAL</a> at the University of Queensland. We also have given references to published research using the methods we discuss.</p><p>LADAL has an overview of text analysis and distant reading. (context?)
Counting Words
More complex methods – classification
More complex methods – others
Visualisation
Counting words
When we count words, what are we trying to discover? Examples?
<br>
<br>
<br></p><h3 id=word-frequency>Word frequency</h3><p>Tracking changes in the frequency of use of words across time has become popular since Google’s n-gram viewer has been available. However, results from this tool have to treated with caution for reasons set out in this blog-post. Expand/summarise the reasons/can this technique be used for other things? Comparing patterns of word frequency across texts can be part of authorship attribution. Patrick Juola describes using this method when he tried to decide whether Robert Galbraith was really J.K Rowling.
<br>
<br>
This paper uses frequency and concordance analysis, with Australian data:
<br>
Bednarek, Monika. 2020. Invisible or high-risk: Computer-assisted discourse analysis of references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes. PLoS ONE 15/6: e0234486. <a href=https://doi.org/10.1371/journal.pone.0234486 target=_blank rel="noopener noreffer">https://doi.org/10.1371/journal.pone.0234486</a>
<br>
<br>
The ratio of types and tokens in a text has been used as a measure of lexical diversity in developmental and clinical studies as well as in stylistics. It has also been applied to theoretical problems in linguistics:
<br>
Kettunen, Kimmo. 2014. Can Type-Token Ratio be Used to Show Morphological Complexity of Languages? Journal of Quantitative Linguistics 21(3). 223–245. <a href=https://doi.org/10.1080/09296174.2014.911506 target=_blank rel="noopener noreffer">https://doi.org/10.1080/09296174.2014.911506</a>.
<br>
<br>
<br></p><h3 id=concordance>Concordance</h3><p>A concordance allows the researcher to see all instances of a word or phrase in a text, neatly aligned in a column and with preceding and following context (see image below).
Concordances are often a first step in analysis. The concordance allows a researcher to see how a word is used and in what contexts. Most concordancing tools allow sorting of results by either preceding or following words – the coloured text in the example below shows that in this case the results have been sorted hierarchically on the three following words. This possibility can help in discovering patterns of co-occurrence. Concordances are also very useful when looking for good examples to illustrate a point. (The type of display seen in the example is often referred to as KeyWord In Context – KWIC. There is a possibility of confusion here, as there is a separate analytic method commonly referred to as Keywords.</p><p>(The example here was produced by Antconc.) check image for clarity
This tutorial from LADAL on concordancing uses a notebook containing R code as a method of extracting concordance data.
<br>
<br>
<br></p><h3 id=clusters-and-collocations>Clusters and collocations</h3><p>Two methods can be used for counting the co-occurrence of items in text.
Clusters (sometimes known as n-grams) are sequences of adjacent items. A bigram is a sequence of two items, a trigram (3-gram) is a sequence of three items and so on. n-grams are types made up of more than one item, and therefore we can count the number of tokens of each n-gram in texts. n-grams are also the basis for a class of language models. (Google created a very large data set of English n-grams in developing their language-based algorithms and this data is available.) Example of n-grams?
Collocations are patterns of co-occurrence in which the items are not necessarily adjacent. This is important because of the structure and use of verbs and their objects in English. The object of a verb is a noun phrase and in many cases the first item in an English noun phrase is a determiner. This means that for many transitive verbs, the bigram *verb the will occur quite frequently. But it is much more interesting to know whether there are patterns relating verbs and the entities which are their objects.
Collocation analysis uncovers such patterns by looking at co-occurrences within a window of a certain size, for example three tokens on either side of the target. Collocation analysis gives information about the frequency of the co-occurrence of words and also a statistical measure of how likely that frequency is, given the overall frequencies of the terms in the corpus. Measures commonly applied include Mutual Information scores and Log-Likelihood scores.
Collocations can also tell us about the meanings of words. If a word has collocates which fall into semantically distinct groups, this can indicate ambiguity or polysemy. And if different words share patterns of collocation, this can be evidence that the words are at least partial synonyms.</p><p>This graphic (potentially too complicated: either an explanation needed or a simpler example) shows collocation relations in Darwin’s Origin of Species visualised as a network:</p><p>This article uses bigram frequencies as part of an analysis of language change:
Schweinberger, Martin. 2021. Ongoing change in the Australian English amplifier system. Australian Journal of Linguistics 41(2). 166–194. <a href=https://doi.org/10.1080/07268602.2021.1931028 target=_blank rel="noopener noreffer">https://doi.org/10.1080/07268602.2021.1931028</a>.
An article which uses concordances and collocation analysis:
Baker, Paul & Tony McEnery. 2005. A corpus-based approach to discourses of refugees and asylum seekers in UN and newspaper texts. Journal of Language and Politics 4(2). 197–226.
This article uses the discovery of shared patterns of collocation as evidence that the words are at least partial synonyms:</p><p>McEnery, Tony & Helen Baker. 2017. Corpus linguistics and 17th-century prostitution: computational linguistics and history (Corpus and Discourse. Research in Corpus and Discourse). London ; New York, NY: Bloomsbury Academic. (especially chapters 4 and 5)</p><p>This tutorial from LADAL on analysing co-occurences and collocations uses a notebook containing R code as a method to extract and visualise semantic links between words.
<br>
<br>
<br></p><h3 id=keywords>Keywords</h3><p>Keyword analysis is a statistically robust method of comparing frequencies of words in corpora. It tells us which words are more frequent (or less frequent) than would be expected in one text compared to another text and gives an estimate of the probability of the result. Keyword analysis uses two corpora: a target corpus, which is the material of interest, and a comparison corpus. Frequency lists are made for each corpus and then frequency of individual types in each corpus are compared. Keywords are ones which occur more frequently in the target corpus than expected given the reference corpus.
The keyness of individual items is a quantitative measure of how unexpected the frequency is; chi-square is a one possible measure of this, but a log-likelihood measure is more commonly used. Positive keywords are …; negative keywords are words which occur less commonly than expected.</p><p>A comparison of distinguishing words for three texts (Charles Darwin’s Origin, Herman Melville’s Moby Dick, and George Orwell’s 1984): What is this an example of? Keyness? Figure needs to be checked for clarity</p><p>This paper applies keyword analysis to Australian text data sourced from a television series script:
Bednarek, Monika. 2020. Keyword analysis and the indexing of Aboriginal and Torres Strait Islander identity: A corpus linguistic analysis of the Australian Indigenous TV drama Redfern Now. International Journal of Corpus Linguistics 25/4: 369-99. <a href=http://doi.org/10.1075/ijcl.00031.bed target=_blank rel="noopener noreffer">http://doi.org/10.1075/ijcl.00031.bed</a>
Tony McEnery describes using the keyword analysis method to compare four varieties of English in this chapter:
McEnery, Tony. 2016. Keywords. In Paul Baker & Jesse Egbert (eds.), Triangulating methodological approaches in corpus-linguistic research (Routledge Advances in Corpus Linguistics 17), 20–32. New York: Routledge.
This article explores how to assess Shakespeare’s use of words to build characters by using keyword analysis of the character’s dialog:
Culpeper, Jonathan. 2002. Computers, language and characterisation: An analysis of six characters in Romeo and Juliet. In Conversation in Life and in Literature: Papers from the ASLA Symposium (Association Suedoise de Linguistique Appliquee (ASLA) 15), 11–30. Uppsala: Universitetstryckeriet. (pdf)
<br>
<br>
<br></p><h3 id=more-complex-methods--classification>More complex methods – Classification</h3><p>Classification methods aim to assign some unit of analysis to a class. For example, a document (or a portion of a document) can be classified as having positive or negative sentiment. These methods are all examples of supervised machine learning. An algorithm is trained on the basis of annotated data to identify classifiers in the data – features which correlate in some way with the annotated classifications. If the algorithm achieves good results on testing data (classified by human judgment), then it can be used to classify unannotated data.
<br>
<br>
<br></p><h3 id=document-classification>Document Classification</h3><p>The task here is to assign documents to categories automatically. What is this used for in the research context?
An common example of this procedure is spam filtering of email that may be applied by internet service providers and also within email applications.
The following two articles taken together give an account of a technique for partially automating the training phase of classification and then of how the classifiers allowed researchers to access new information in a large and complex text.
Leavy, Susan, Mark T Keane & Emilie Pine. 2019. Patterns in language: Text analysis of government reports on the Irish industrial school system with word embedding. Digital Scholarship in the Humanities 34(Supplement_1). i110–i122. <a href=https://doi.org/10.1093/llc/fqz012 target=_blank rel="noopener noreffer">https://doi.org/10.1093/llc/fqz012</a>.</p><p>Pine, Emilie, Susan Leavy & Mark T. Keane. 2017. Re-reading the Ryan Report: Witnessing via and Close and Distant Reading. Éire-Ireland 52(1–2). 198–215. <a href=https://doi.org/10.1353/eir.2017.0009 target=_blank rel="noopener noreffer">https://doi.org/10.1353/eir.2017.0009</a>. (available online)</p><p>[Wikipedia]
<br>
<br>
<br></p><h3 id=sentiment-analysis>Sentiment analysis</h3><p>Sentiment analysis assigns (assigned to what? Could “sorts” be better?) documents according to the affect which they express. In simple cases, this can mean dividing (sorting?) documents into those which a express a positive view and those which express a negative view (with a neutral position sometimes also included). Those classifications are the basis for aggregated ratings - for example, online listings of movies and restaurants). A sentiment value is assigned to individual reviews then an aggregate score is calculated based on those values. The score then becomes … More sophisticated sentiment analysis can assign values on a scale. Some sentiment analysis tools use dictionaries of terms with sentiment values assigned to those terms; these are known as pre-trained or pre-determined classifiers.
The following figure shows the results of the sentiment analysis of four texts (The Adventures of Huckleberry Finn by Mark Twain, 1984 by George Orwell, The Colour out of Space by H.P.Lovecraft, and On the Origin of Species by Charles Darwin) using the Word-Emotion Association Lexicon (Mohammad and Turney 2013): (check graphic for clarity)</p><p>The Wikipedia entry for Sentiment Analysis gives more information and examples particularly in relation to the use of sentiment analysis as a tool used in online settings.</p><p>LADAL’s Sentiment Analysis tutorial uses a notebook containing R code as a method of performing sentiment analysis.
This article discusses problems in assembling training data for complex sentiment analysis tasks and then applies the results to oral history interviews with Holocaust survivors:
Blanke, Tobias, Michael Bryant & Mark Hedges. 2020. Understanding memories of the Holocaust—A new approach to neural networks in the digital humanities. Digital Scholarship in the Humanities 35(1). 17–33. <a href=https://doi.org/10.1093/llc/fqy082 target=_blank rel="noopener noreffer">https://doi.org/10.1093/llc/fqy082</a>.
<br>
<br>
<br></p><h3 id=named-entity-recognition>Named Entity Recognition</h3><p>Named Entity Recognition involves two levels of classification. First, segments of text are classified as either denoting or not denoting an entity: a person, a place or an organization. These entities can then be classified as belonging to a particular type.
The Wikipedia entry explaining named-entity recognition gives further detail about the technique.
This article looks at the problems encountered when applying a well-known entity recognition package (Stanford) to historical newspapers in the National Library of Australia’s Trove collection:
Mac Kim, Sunghwan & Steve Cassidy. 2015. Finding names in trove: named entity recognition for Australian historical newspapers. In Proceedings of the Australasian Language Technology Association Workshop 2015, 57–65. (pdf)
This article (section 6.3) discusses why entity recognition is not as useful as might be expected when studying names in novels: [title is kind of confusing with the next section being call computational stylistics also]
Dalen-Oskam, K. van. 2013. Names in novels: An experiment in computational stylistics. Literary and Linguistic Computing 28(2). 359–370. <a href=https://doi.org/10.1093/llc/fqs007 target=_blank rel="noopener noreffer">https://doi.org/10.1093/llc/fqs007</a>.
<br>
<br>
<br></p><h3 id=computational-stylistics-stylometry>Computational Stylistics (Stylometry)</h3><p>This is used to evaluate and analyse an author’s writing style by identifying their unique patterns of language use. It is also referred to as authorship attribution. [Why is this important?] The classification task is deciding whether a piece of text can be attributed to a particular author (and with what degree of confidence) [how? because? Based on what?]. Seemingly simple classifiers [such as?] are used for this task as they are assumed to be less open to conscious manipulation by writers. For example, comparative patterns of occurrence of function words [example] are considered a better classifier than occurrences of content words. Character n-grams have also proven to be a good classifier for use in this task.
The Wikipedia entry on stylometry gives further information on the methodology.
A classic stylometric study using Bayesian statistics rather than machine learning is:
Mosteller, Frederick & David Lee Wallace. 1984. Applied Bayesian and classical inference: the case of the Federalist papers. New York: Springer-Verlag.
This article applies stylometric techniques to a classic of Chinese literature:
Zhu, Haoran, Lei Lei & Hugh Craig. 2021. Prose, Verse and Authorship in Dream of the Red Chamber : A Stylometric Analysis. Journal of Quantitative Linguistics 28(4). 289–305. <a href=https://doi.org/10.1080/09296174.2020.1724677 target=_blank rel="noopener noreffer">https://doi.org/10.1080/09296174.2020.1724677</a>.
An overview of the use of function words in stylometry:
Garcia, A. M. & J. C. Martin. 2007. Function Words in Authorship Attribution Studies. Literary and Linguistic Computing 22(1). 49–66. <a href=https://doi.org/10.1093/llc/fql048 target=_blank rel="noopener noreffer">https://doi.org/10.1093/llc/fql048</a>.
<br>
<br>
<br></p><h3 id=more-complex-methods--others>More complex methods – Others</h3><p>Topic models
Topic modeling is a method which tries to recover [discover?] abstract ‘topics’ which occur in a collection of documents. The underlying theory is that different topics will tend to be associated with different words, and that different documents will tend to be associated with different topics. The complete model includes [outputs?] the strength of association (or probability) between each word and each topic, and between each topic and each document. A topic consists of a group of words and it is up to the researcher to decide if a semantically coherent interpretation can be given to any [identified?] topic.
The below example visualisation is based on topic modeling of the State of the Union addresses given by US presidents, and shows the relative importance of different topics over time:</p><p>The Wikipedia entry for topic models gives a more detailed explanation of the process.</p><p>This topic modeling tutorial from LADAL uses R coding to process textual data and generate a topic model from that data.
Poetics 41(6) is a journal issue devoted to the use of topic models in literary studies: the introduction to the journal (by Mohr and Bogdanov) provides a useful overview of the method.
And this paper uses topic modeling as one tool in trying to improve access to a huge collection of scholarly literature:
Mimno, David. 2012. Computational historiography: Data mining in a century of classics journals. Journal on Computing and Cultural Heritage 5(1). 1–19. <a href=https://doi.org/10.1145/2160165.2160168 target=_blank rel="noopener noreffer">https://doi.org/10.1145/2160165.2160168</a>.
<br>
<br>
<br></p><h3 id=network-analysis>Network Analysis</h3><p>Network analysis allows us to produce visualisations of the relationships between entities within a dataset. Analysis of social networks is a classic application of the method, but words and documents can also be thought of as entities and the relationships between them can then be analysed with this method. (see example visualisation of Darwin’s Origin of Species above)
Here is another example of a network graph illustrating the relationships between the characters of Shakespeare’s Romeo and Juliet: [check graphic for clarity]</p><p>This article gives several examples of how representing collocational link between words as a network can lead to insight into meaning relations:
Brezina, Vaclav, Tony McEnery & Stephen Wattam. 2015. Collocations in context: A new perspective on collocation networks. International Journal of Corpus Linguistics 20(2). 139–173. <a href=https://doi.org/10.1075/ijcl.20.2.01bre target=_blank rel="noopener noreffer">https://doi.org/10.1075/ijcl.20.2.01bre</a>. (pdf)
Wikipedia has several good articles on network theory in general and social network analysis
in particular.</p><p>LADAL’s tutorial on Network Analysis introduces this method using R coding.
<br>
<br>
<br></p><h3 id=visualisation>Visualisation</h3><p>Visualisation is an important technique for exploring data, allowing us to see patterns easily, and also for presenting results.
There are many methods for creating visualisations and this article gives an overview of some possibilities for visualising corpus data:
Siirtola, Harri, Terttu Nevalainen, Tanja Säily & Kari-Jouko Räihä. 2011. Visualisation of text corpora: A case study of the PCEEC. How to Deal with Data: Problems and Approaches to the Investigation of the English Language over Time and Space. Helsinki: VARIENG 7. [html]
<br>
<br>
If you want to see something more complex, this article includes animations showing change in use of semantic space over time – but you need to have full access to the online publication to see it. [Should we be sending people to things that they may not be able to access?]
<br>
<br>
Hilpert, Martin & Florent Perek. 2015. Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts. Linguistics Vanguard 1(1). <a href=https://doi.org/10.1515/lingvan-2015-0013 target=_blank rel="noopener noreffer">https://doi.org/10.1515/lingvan-2015-0013</a>.
This LADAL tutorial on data visualisation in R makes use of the ggplot package to create some common data visualisation using code.</p></div></div></div></main><footer class=footer>
<div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.93.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.10">Based on <i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
</div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=http://www.atap.edu.au target=_blank>Australian Text Analytics Platform</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top">
<i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments">
<i class="fas fa-comment fa-fw"></i>
</a>
</div><script type=text/javascript src=/ATAP-website/lib/smooth-scroll/smooth-scroll.min.js></script><script type=text/javascript src=/ATAP-website/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/ATAP-website/lib/lunr/lunr.min.js></script><script type=text/javascript src=/ATAP-website/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/ATAP-website/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/ATAP-website/lib/sharer/sharer.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},search:{highlightTag:"em",lunrIndexURL:"/ATAP-website/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/ATAP-website/js/theme.min.js></script></body></html>