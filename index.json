[{"categories":null,"content":"The ATAP workbench will enable researchers to work in ways which improve the reproducibility and accountability of their research. To do this, the output of your work in ATAP will be a fully documented research object. Research objects are a single package that collects the data, code and other resources used in a piece of research and allows it to be a cite-able research output in its own right. At the end of your work in ATAP, there will be the automated option to package together the components of your workflow, such as: Raw data Transformed data A record of the notebooks which you have used Additional scripts and codes Results Visualisations High quality metadata ATAP will output an RO-Crate that contains these objects. An output of this kind can be assigned a unique identifier (such as a doi), and it can be published via services such as Zenodo or figshare so it can be cited in publications. This will make it easy to fulfil the increasingly common requirement of journals to make available the data that supports a publication. ","date":"15 Feb 2022","objectID":"/research_objects/:0:0","tags":null,"title":"Research Objects","uri":"/research_objects/"},{"categories":null,"content":"Understanding Text as Data Many definitions of data include an element such as individual items of information. If we consider text to include any sort of language in use, covering different modalities (spoken, written signed) and different extents (from individual sounds to multi-volume books), then fitting text to this definition requires some abstraction. We have to define some unit or units of analysis and we can then treat each of those units as an individual item of information. Examples of such units include documents, sentences and words. But word is not as simple a unit as you may think. ","date":"15 Feb 2022","objectID":"/text_analysis/:0:1","tags":null,"title":"Text Analysis Overview","uri":"/text_analysis/"},{"categories":null,"content":"What’s in a word? The term word is problematic. It is well-known to linguists that phonological words (defined by sound patterns), syntactic words (defined by combinatorial possibilities) and orthographic words (defined by the conventions of a writing system) do not always coincide. And even when we are only looking at written material, there are problems. How many words are there in this sentence? The cat sat on the mat One answer is that there are six words; that is, there are six groups of characters which are separated according to typographical convention. But there is another answer: There are five words, that is five distinct sequences of characters and one of those sequences (the) occurs twice. The terms standardly used to make this distinction are type and token. Tokens are instances of types, therefore if we count tokens, we count without considering repetition, while if we count types, we do consider repetition. In our example, there are five types (the, cat, sat, on, mat) but six tokens, because there are two tokens of one of the types (the). There is a further distinction we may need to make which we can see if we consider another question: Are cat and cats the same word? They are distinct types, and therefore must also be distinct as tokens. But we have an intuition that at some level they are related, that there is some more abstract item which underlies both of them. This concept is usually referred to as a lemma (there is more about this concept on the Data Preparation page). ","date":"15 Feb 2022","objectID":"/text_analysis/:0:2","tags":null,"title":"Text Analysis Overview","uri":"/text_analysis/"},{"categories":null,"content":"Text Analysis Workflow This brief introduction to text analysis divides the process into three parts. In the first stage, the text is made into data. It is divided into the units appropriate for the analysis to be carried out and shaped to a format which our analytic tools can work with. The second stage is the analysis proper, including its interpretation. A wide range of analytic methods can be used, and we give a survey of some of the commonly used possibilities. Finally, our data, methods and results can be documented and packaged as a research object which can be stored and reused. Data Preparation Useful Methods Research Objects ","date":"15 Feb 2022","objectID":"/text_analysis/:0:3","tags":null,"title":"Text Analysis Overview","uri":"/text_analysis/"},{"categories":null,"content":"Webinars Forthcoming workshops Previous workshops ","date":"15 Feb 2022","objectID":"/events/:0:0","tags":null,"title":"Events","uri":"/events/"},{"categories":null,"content":"Webinars Our webinar series is a joint initiative with the Language Technology and Data Analysis Laboratory (LADAL), (School of Languages and Cultures, University of Queensland). LADAL sponsored webinars take place in the alternate months. All webinars take place at 8:00PM Brisbane time which is UTC+10. Zoom links will be available one week prior to the event. 4 April 2022 - Keoni Mahelona: A practical approach to Indigenous data sovereignty Keoni Mahelona is the Chief Technical Officer of Te Hiku Media where he is a part of the team developing the Kaitiakitanga Licence. This licence seeks to balance the importance of publicly accessible data with the reality that indigenous peoples may not have access to the resources that enable them to benefit from public data. By simply opening access to data and knowledge, indigenous people could be further colonised and taken advantage of in a digital, modern world. Therefore Keoni is committed to devising data governance regimes which enable Indigenous people to reclaim and maintain sovereignty over indigenous data. June 6 2022 - Barbara McGillivray: The Journal of Open Humanities Data Barbara McGillivray is a Turing Research Fellow at The Alan Turing Institute, and Editor in Chief of the Journal of Open Humanities Data. Since September 2021 she is also a lecturer in Digital Humanities and Cultural Computation at the Department of Digital Humanities of King’s College London. Before joining the Turing, she was language technologist in the Dictionary division of Oxford University Press and data scientist in the Open Research Group of Springer Nature. Her research at the Turing is on how words change meaning over time and how to model this change in computational ways. She works on machine-learning models for the change in meaning of words in historical times (Ancient Greek, Latin, eighteen-century English) and in contemporary texts (Twitter, web archives, emoji). Her interdisciplinary contribution covers Data Science, Natural Language Processing, Historical Linguistics and other humanistic fields, to push the boundaries of what academic disciplines separately have achieved so far on this topic. August 1 2022 - Václav Cvrček: The Czech national Corpus Václav Cvrček is a linguist who deals with the description of the Czech language, especially with the use of large electronic corpora and quantitative methods. In 2013-2016 he worked as the director of the Czech National Corpus project, since 2016 he has been the deputy director. Recently, he has been focusing on research on textual variability and corpus-based discourse analysis with a focus on online media. October 3 2022 - Paweł Kamocki: [topic tba] Paweł Kamocki is a legal expert in Leibniz-Institut für Deutsche Sprache, Mannheim. He studied linguistics and law, and in 2017 obtained his doctorate in law from the universities of Paris and Münster for a thesis on legal aspects of data-intensive university research, with a focus on Knowledge Commons. He worked as a research and teaching assistant at the Paris Descartes university (now: Université de Paris), then also in the private sector. He is certified to work as an attorney in France. An active member of the CLARIN community since 2012, he currently chairs the CLARIN Legal and Ethical Issues Committee. He also worked with other projects and initiatives in the field of research data policy (RDA, EUDAT) and co-created several LegalTech tools for researchers. One of his main research interests are legal issues in Machine Translation. ","date":"15 Feb 2022","objectID":"/events/:0:1","tags":null,"title":"Events","uri":"/events/"},{"categories":null,"content":"Forthcoming workshops 16 March 2022 - Monotreme Mania! Comparative text analytics on Twitter data This online workshop is offered free to Australian researchers and research students and will cover: Collecting and transforming Twitter data using twarc. Analysing the collected Twitter data using the R ecosystem for text analytics. Register via Eventbrite Update - 09/03/2022 The workshop is now fully booked. If you missed out, do check back next week in case of cancellations, and let us know if you’re keen for us to run the workshop again later. Brought to you by the teams at the Australian Digital Observatory (ADO) and the Australian Text Analytics Platform (ATAP) via the Australian Research Data Commons (ARDC). ","date":"15 Feb 2022","objectID":"/events/:0:2","tags":null,"title":"Events","uri":"/events/"},{"categories":null,"content":"Previous workshops An introduction to Jupyter notebooks for text analysis: Virtual workshop for absolute beginners Date: 24 November 2021 Event: Digital Humanities Australasia 2021 Conference Length: 3 hours Facilitators: Sara King, Simon Musgrave ","date":"15 Feb 2022","objectID":"/events/:0:3","tags":null,"title":"Events","uri":"/events/"},{"categories":null,"content":"ATAP is one strand of the partnership between the Australian Research Data Commons (ARDC) and the School of Languages and Cultures at the University of Queensland. This partnership includes a number of projects that explore language-related technologies, data collection infrastructure and Indigenous capability programs. These projects are being led out of the Language Technology and Data Analytics Lab (LADAL), which is overseen by Professor Michael Haugh and Dr Martin Schweinberger. ","date":"15 Feb 2022","objectID":"/organisation/:0:0","tags":null,"title":"Organisation","uri":"/organisation/"},{"categories":null,"content":"Partner Institutions: University of Queensland: Professor Michael Haugh University of Sydney: Professor Monika Bednarek (Sydney Corpus Lab) AARNet Dr Sara King Ryan Fraser ","date":"15 Feb 2022","objectID":"/organisation/:1:0","tags":null,"title":"Organisation","uri":"/organisation/"},{"categories":null,"content":"Project Team ","date":"15 Feb 2022","objectID":"/organisation/:2:0","tags":null,"title":"Organisation","uri":"/organisation/"},{"categories":null,"content":"Technology and Interoperability Team Lead: Peter Sefton Moises Sacal Marco La Rosa Michael D’Silva (AARNet) River Smith Alvin Sebastian ","date":"15 Feb 2022","objectID":"/organisation/:2:1","tags":null,"title":"Organisation","uri":"/organisation/"},{"categories":null,"content":"Applications and Training Team Lead: Ben Foley Michael Niemann Marius Mathers (Sydney Informatics Hub) ","date":"15 Feb 2022","objectID":"/organisation/:2:2","tags":null,"title":"Organisation","uri":"/organisation/"},{"categories":null,"content":"Data and Policy Team Lead: Kathrin Kaiser Cale Johnstone ","date":"15 Feb 2022","objectID":"/organisation/:2:3","tags":null,"title":"Organisation","uri":"/organisation/"},{"categories":null,"content":"Engagement and Outreach Team Lead: Simon Musgrave Sara King (AARNet) Leah Gustafson Harriet Sheppard ","date":"15 Feb 2022","objectID":"/organisation/:2:4","tags":null,"title":"Organisation","uri":"/organisation/"},{"categories":null,"content":"Project Manager: Marco Fahmi ","date":"15 Feb 2022","objectID":"/organisation/:2:5","tags":null,"title":"Organisation","uri":"/organisation/"},{"categories":null,"content":"Project Coordinator: Leah Gustafson ","date":"15 Feb 2022","objectID":"/organisation/:2:6","tags":null,"title":"Organisation","uri":"/organisation/"},{"categories":null,"content":"Introducing data preparation concepts What data scientists spend the most time doing - pie chart This graphic is, sadly, all too true. Data scientists and those who are using data as part of their research spend much of their time preparing their dataset and transforming its structure into a format that can be used (often referred to as data wrangling or data munging). The Australian Text Analytics Platform will offer a range of tools to assist in cleaning text data and performing other preliminary operations which can prepare the data for analysis. ATAP analysis notebooks assume a common data structure, however the platform will provide notebooks containing code for transforming data into the structure that is needed for the procedure(s) in the analysis notebooks. There are two main processes that are needed to prepare text data for analysis, cleaning and annotation, and the ones that you will need to use will depend on the dataset that you are using. ","date":"15 Feb 2022","objectID":"/data_prep/:0:1","tags":null,"title":"Preparing Text Data","uri":"/data_prep/"},{"categories":null,"content":"Common Cleaning Techniques Making all of the text lower case: This ensures that e.g. dog and Dog will not be treated as different items and is important if you are going to use analytic methods which rely on counting items. However, if you are planning to extract entities from your text data, retaining capital letters may be important. Standardising spelling: At least for English text, there are some well-known spelling variations, some with a geographical context (colour/color) and some that are more a matter of personal preference (recognise/recognize). As with case, standardising spelling ensures that pairs like the examples are treated as tokens of the same type. Removing stopwords: Stopwords are function words that are not interesting for many analyses and we can safely remove them from our data using a stoplist. The 20 most frequently occurring words in the British National Corpus are the, of, and, a, in, to, it, is, was, to, I, for, you, he, be, with, on, that, by, and at. The equivalent list for the Corpus of Contemporary American English (COCA) is the, be, and, of, a, in, to, have, to, it, I, that, for, you, he, with, on, do, say and this. Some of the differences between the two are because the COCA counts are of lemmas (see Lemmatization below) which are the base forms of a word (think dog and dogs). Packages such as nltk and spaCy include standard stoplists for various languages, and it is possible to specify other words to be excluded. Removing punctuation: Punctuation can change how a text analysis package identifies a word. For instance to be sure that dog and dog? are not treated as different items, removing punctuation is good practice. Removing numbers: Sometimes the presence of numbers in documents can lead to artefacts in analysis. For example, in a collection of documents with page numbering, the numbers might show up as collocates of words or as part of a topic in a topic model. To avoid this, removing numbers is also good practice. This can present a challenge where numbers might be of interest (e.g. a study of mathematics textbooks). Removing whitespace: Whitespace can be another possible source of artefacts in analysis, especially if the source material uses a lot of tabs. ","date":"15 Feb 2022","objectID":"/data_prep/:0:2","tags":null,"title":"Preparing Text Data","uri":"/data_prep/"},{"categories":null,"content":"Annotation Annotation is the process of adding information to your base dataset in order to make it possible to apply analytic techniques. In some cases, this may be a manual process. For example, much of the annotation which is described in the Text Encoding Initiative Guidelines requires a human making decisions although, in some cases, manual annotation processes may also be scaled up to large text corpora using text classification or information extraction technologies. However some annotation can be carried out automatically, and there are two important kinds of annotation for text which fall into this category. Part-of-speech tagging (POS-tagging): For some analytic procedures, knowing the part of speech (or class of words) that an item belongs to is important. For languages where good pre-trained models exist, this annotation can be carried out automatically to a high level of accuracy – for English, we expect an accuracy rate better than 95%. POS-taggers generally provide more information than just whether an item is a noun or a verb, they also distinguish singular and plural forms of nouns and tell us whether a verb’s form is present tense form or a past tense. The tag sets which are used can therefore be quite extensive, and there are various tag sets in use such as the Penn Treebank tags and the CLAWS tags used by the British National Corpus.  LADAL has some excellent resources that discuss POS tagging in more detail. Lemmatization: The distinctions between different forms of a single lexeme can be a hindrance in analysis especially if we are interested in lexical semantics in texts. Lemmatization identifies the base forms of words (or lemmas) in a text so that all forms of an item are treated together. For example: dog and dogs will both be instances of the lemma DOG. eat, eats, eating and ate will all be treated as tokens of the lemma EAT. As noted above, POS-tags give information about the form of words and are generally part of the annotation in lemmatization. A lemma, along with a POS-tag, can be reconstructed to the original form if necessary. ","date":"15 Feb 2022","objectID":"/data_prep/:0:3","tags":null,"title":"Preparing Text Data","uri":"/data_prep/"},{"categories":null,"content":" Language Technology and Data Analysis Laboratory (LADAL) LADAL aims to help develop computational and digital skills by providing information and practical, hands-on tutorials on data and text analytics as well as on statistical methods relevant for language research. GLAM Workbench The GLAM workbench is a collection of tools, tutorials, examples, and hacks created by Tim Sherratt to help you work with data from galleries, libraries, archives, and museums (the GLAM sector). The primary focus is Australia and New Zealand, but new collections are being added all the time. CONSTELLATE Constellate is the text analytics service from the not-for-profit ITHAKA - the same people who brought you JSTOR and Portico. It is a platform for teaching, learning, and performing text analysis using the world’s leading archival repositories of scholarly and primary source content. The Art of Literary Text Analysis The Art of Literary Text Analysis (ALTA) has three objectives. First, to introduce concepts and methodologies for literary text analysis programming. It doesn’t assume you know how to program or how to use digital tools for analyzing texts. Second, to show a range of analytical techniques for the study of texts. While it cannot explain and demonstrate everything, it provides a starting point for humanists with links to other materials. Third, to provide utility notebooks you can use for operating on different texts. These are less well documented and combine ideas from the introductory notebooks. Introduction to Cultural Analytics \u0026 Python is an online textbook by Melanie Walsh, which offers an introduction to the programming language Python that is specifically designed for people interested in the humanities and social sciences. This book demonstrates how Python can be used to study cultural materials such as song lyrics, short stories, newspaper articles, tweets, Reddit posts, and film screenplays. It also introduces computational methods such as web scraping, APIs, topic modeling, Named Entity Recognition (NER), network analysis, and mapping. Text Analysis Pedagogy Institute is an open educational institute for the benefit of teachers (and aspiring teachers) of text analysis in the digital humanities. The Programming Historian publishes novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate research and teaching. Quinn Dombrowski’s list of relevant courses and tutorials A collection of Jupyter notebooks in many human and computer languages for doing digital humanities. ","date":"15 Feb 2022","objectID":"/resources/:0:0","tags":null,"title":"Resources","uri":"/resources/"},{"categories":null,"content":"Counting words More complex methods - Classification More complex methods - Others Visualisation ","date":"15 Feb 2022","objectID":"/methods/:0:0","tags":null,"title":"Useful Methods","uri":"/methods/"},{"categories":null,"content":"Introduction Throughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory LADAL at the University of Queensland. We also have given references to published research using the methods we discuss. LADAL has an overview of text analysis and distant reading. ","date":"15 Feb 2022","objectID":"/methods/:0:1","tags":null,"title":"Useful Methods","uri":"/methods/"},{"categories":null,"content":"Counting Words Word frequency Knowing how frequently words occur in a text can already give us information about that text and frequency lists based on large corpora are a useful tool in themselves - you can download such lists for the (original) British National Corpus. Tracking changes in the frequency of use of words across time has become popular since Google’s n-gram viewer has been available. However, results from this tool have to treated with caution for reasons set out in this blog-post. Comparing patterns of word frequency across texts can be part of authorship attribution. Patrick Juola describes using this method when he tried to decide whether Robert Galbraith was really J.K Rowling. This paper uses frequency and concordance analysis, with Australian data: Bednarek, Monika. 2020. Invisible or high-risk: Computer-assisted discourse analysis of references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes. PLoS ONE 15/6: e0234486. https://doi.org/10.1371/journal.pone.0234486 The ratio of types and tokens in a text has been used as a measure of lexical diversity in developmental and clinical studies as well as in stylistics. It has also been applied to theoretical problems in linguistics: Kettunen, Kimmo. 2014. Can Type-Token Ratio be Used to Show Morphological Complexity of Languages? Journal of Quantitative Linguistics 21(3). 223–245. https://doi.org/10.1080/09296174.2014.911506. Concordance A concordance allows the researcher to see all instances of a word or phrase in a text, neatly aligned in a column and with preceding and following context (see image below). Concordances are often a first step in analysis. The concordance allows a researcher to see how a word is used and in what contexts. Most concordancing tools allow sorting of results by either preceding or following words – the coloured text in the example below shows that in this case the results have been sorted hierarchically on the three following words. This possibility can help in discovering patterns of co-occurrence. Concordances are also very useful when looking for good examples to illustrate a point. (The type of display seen in the example is often referred to as KeyWord In Context – KWIC. There is a possibility of confusion here, as there is a separate analytic method commonly referred to as Keywords.) (The example here was produced by Antconc) This tutorial from LADAL on concordancing uses a notebook containing R code as a method of extracting concordance data. Clusters and collocations Two methods can be used for counting the co-occurrence of items in text. Clusters (sometimes known as n-grams) are sequences of adjacent items. A bigram is a sequence of two items, a trigram (3-gram) is a sequence of three items and so on. n-grams are types made up of more than one item, and therefore we can count the number of tokens of each n-gram in texts. n-grams are also the basis for a class of language models. (Google created a very large data set of English n-grams in developing their language-based algorithms and this data is available.) Collocations are patterns of co-occurrence in which the items are not necessarily adjacent. An example of why this is important is verbs and their objects in English. The object of a verb is a noun phrase and in many cases the first item in an English noun phrase is a determiner. This means that for many transitive verbs, the bigram verb the will occur quite frequently. But it is much more interesting to know whether there are patterns relating verbs and the entities which are their objects. Collocation analysis uncovers such patterns by looking at co-occurrences within a window of a certain size, for example three tokens on either side of the target. Collocation analysis gives information about the frequency of the co-occurrence of words and also a statistical measure of how likely that frequency is, given the overall frequencies of the terms in the corpus. Measures com","date":"15 Feb 2022","objectID":"/methods/:0:2","tags":null,"title":"Useful Methods","uri":"/methods/"},{"categories":null,"content":"More complex methods – Classification Classification methods aim to assign some unit of analysis, such as a word or a document, to a class. For example, a document (or a portion of a document) can be classified as having positive or negative sentiment. These methods are all examples of supervised machine learning. An algorithm is trained on the basis of annotated data to identify classifiers in the data – features which correlate in some way with the annotated classifications. If the algorithm achieves good results on testing data (classified by human judgment), then it can be used to classify unannotated data. Document Classification The task here is to assign documents to categories automatically. An everyday example of this procedure is spam filtering of email that may be applied by internet service providers and also within email applications. An example of this technique being used in research would be automatically identifying historical court records as referring either to violent crimes, property offences, or other crimes. The following two articles taken together give an account of a technique for partially automating the training phase of classification and then of how the classifiers allowed researchers to access new information in a large and complex text. Leavy, Susan, Mark T Keane \u0026 Emilie Pine. 2019. Patterns in language: Text analysis of government reports on the Irish industrial school system with word embedding. Digital Scholarship in the Humanities 34(Supplement_1). i110–i122. https://doi.org/10.1093/llc/fqz012. Pine, Emilie, Susan Leavy \u0026 Mark T. Keane. 2017. Re-reading the Ryan Report: Witnessing via Close and Distant Reading. Éire-Ireland 52(1–2). 198–215. https://doi.org/10.1353/eir.2017.0009. (available online) Wikipedia Sentiment analysis Sentiment analysis assigns documents according to the affect which they express. In simple cases, this can mean sorting documents into those which a express a positive view and those which express a negative view (with a neutral position sometimes also included). Such classifications are the basis for aggregated ratings - for example, online listings of movies and restaurants. A sentiment value is assigned to individual reviews then an aggregate score is calculated based on those values and that aggregate score is the rating presented to the user. More sophisticated sentiment analysis can assign values on a scale. Some sentiment analysis tools use dictionaries of terms with sentiment values assigned to those terms; these are known as pre-trained or pre-determined classifiers. The following figure shows the results of the sentiment analysis of four texts (The Adventures of Huckleberry Finn by Mark Twain, 1984 by George Orwell, The Colour out of Space by H.P.Lovecraft, and On the Origin of Species by Charles Darwin) using the Word-Emotion Association Lexicon (Mohammad and Turney 2013). The graphic shows what percentage of each text can be assigned to each of eight categories of sentiment: The Wikipedia entry for Sentiment Analysis gives more information and examples particularly in relation to the use of sentiment analysis as a tool used in online settings. LADAL’s Sentiment Analysis tutorial uses a notebook containing R code as a method of performing sentiment analysis. This article discusses problems in assembling training data for complex sentiment analysis tasks and then applies the results to oral history interviews with Holocaust survivors: Blanke, Tobias, Michael Bryant \u0026 Mark Hedges. 2020. Understanding memories of the Holocaust—A new approach to neural networks in the digital humanities. Digital Scholarship in the Humanities 35(1). 17–33. https://doi.org/10.1093/llc/fqy082. Named Entity Recognition Named Entity Recognition involves two levels of classification. First, segments of text are classified as either denoting or not denoting an entity: for example, a person, a place or an organization. The identified entities can then be classified as belonging to one ","date":"15 Feb 2022","objectID":"/methods/:0:3","tags":null,"title":"Useful Methods","uri":"/methods/"},{"categories":null,"content":"More complex methods – Others Topic models Topic modeling is a method which tries to recover abstract ‘topics’ which occur in a collection of documents. The underlying assumption is that different topics will tend to be associated with different words, different documents will tend to be associated with different topics, and therefore the distribution of words across documents allows us to find topics. The complete model includes the strength of association (or probability) between each word and each topic, and between each topic and each document. A topic consists of a group of words and it is up to the researcher to decide if a semantically coherent interpretation can be given to any of the topics recovered. The number of topics to be recovered is specified in advance. The example visualisation below is based on topic modeling of the State of the Union addresses given by US presidents, and shows the relative importance of different topics over time. In the right hand part of the figure, the words most closely linked to each topic are listed; the researcher has not attempted to give labels to these (although in some cases, it is not too hard to imagine what labels we might use). Note also that words are not uniquely linked to topics - for example, the word state is closely linked to seven of the topics in this model. The Wikipedia entry for topic models gives a more detailed explanation of the process. This topic modeling tutorial from LADAL uses R coding to process textual data and generate a topic model from that data. Poetics 41(6) is a journal issue devoted to the use of topic models in literary studies: the introduction to the journal (by Mohr and Bogdanov: https://doi.org/10.1016/j.poetic.2013.10.001) provides a useful overview of the method. And this paper uses topic modeling as one tool in trying to improve access to a huge collection of scholarly literature: Mimno, David. 2012. Computational historiography: Data mining in a century of classics journals. Journal on Computing and Cultural Heritage 5(1). 1–19. https://doi.org/10.1145/2160165.2160168. Network Analysis Network analysis allows us to produce visualisations of the relationships between entities within a dataset. Analysis of social networks is a classic application of the method, but words and documents can also be thought of as entities and the relationships between them can then be analysed with this method. (see example visualisation of Darwin’s Origin of Species above) Here is another example of a network graph illustrating the relationships between the characters of Shakespeare’s Romeo and Juliet: This article gives several examples of how representing collocational links between words as a network can lead to insight into meaning relations: Brezina, Vaclav, Tony McEnery \u0026 Stephen Wattam. 2015. Collocations in context: A new perspective on collocation networks. International Journal of Corpus Linguistics 20(2). 139–173. https://doi.org/10.1075/ijcl.20.2.01bre. (pdf) Wikipedia has articles on network theory in general and on social network analysis. in particular. LADAL’s tutorial on Network Analysis introduces this method using R coding. ","date":"15 Feb 2022","objectID":"/methods/:0:4","tags":null,"title":"Useful Methods","uri":"/methods/"},{"categories":null,"content":"Visualisation Visualisation is an important technique for exploring data, allowing us to see patterns easily, and also for presenting results. There are many methods for creating visualisations and this article gives an overview of some possibilities for visualising corpus data: Siirtola, Harri, Terttu Nevalainen, Tanja Säily \u0026 Kari-Jouko Räihä. 2011. Visualisation of text corpora: A case study of the PCEEC. How to Deal with Data: Problems and Approaches to the Investigation of the English Language over Time and Space. Helsinki: VARIENG 7. [html] If you would like to see something more complex, this article includes animations showing change in use of semantic space over time – but you need to have full access to the online publication to see it. Hilpert, Martin \u0026 Florent Perek. 2015. Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts. Linguistics Vanguard 1(1). https://doi.org/10.1515/lingvan-2015-0013. This LADAL tutorial on data visualisation in R makes use of the ggplot2 package to create some common data visualisations using code. ","date":"15 Feb 2022","objectID":"/methods/:0:5","tags":null,"title":"Useful Methods","uri":"/methods/"},{"categories":null,"content":"Written by Simon Musgrave Data is becoming increasingly important in today’s world, so corpus linguists might feel that the rest of the world is finally catching up. But the rest of the world are bringing with them new approaches to how data is handled. This means that fields such as corpus linguistics may need to reassess their practices. Such reassessment includes addressing concerns about how data is stored and who can access it (data stewardship) – concerns that are a part of the Open Science movement, ultimately grounded on principles of equity and accountability. The most influential approach to data stewardship today is the FAIR principles. According to these principles, data should be: Findable Metadata and data should be easy to find for both humans and computers. Accessible Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation. Interoperable The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing. Reusable The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings. In general, corpus linguists do well on the interoperability criterion. Corpus data is usually stored in non-proprietary formats; even when some structure is imposed on the data, this is almost always in a form which is saved as a simple text file (e.g. csv files or xml annotations). Data stored in such formats is easy to move between applications. But what about the other three criteria? Some corpus data is easy to discover; it is findable. For example CLARIN, the portal to the European Union language resource infrastructure, provides access to many large data collections, as does the Linguistic Data Consortium in the USA. However, some data is never made part of a large collection and often remains under the control of individual researchers or research teams. Such data may be almost impossible to find. Even if we can find such data, it is unlikely to be accompanied by good descriptions of the data and metadata, making reusability problematic. Of course, big corpora such as the British National Corpus will be both findable and accompanied by comprehensive corpus manuals. However, it is worth considering how to make other corpora more findable, including the provision of corpus manuals or corpus descriptions. Corpus resource databases such as CoRD do aim to work towards this principle. Accessibility may also be an issue for some data. Copyright law may allow use of material for individual research but prohibit any further distribution of the material. The FAIR approach to such cases is that metadata should be available so that interested parties can know that a data holding exists (F), and the metadata will include information about the conditions under which the data may or may not be shared or reused (A and R). Image from Global Indigenous Data Alliance (https://www.gida-global.org/) For linguists, there is another very important set of principles concerning data, the CARE principles developed by the Global Indigenous Data Alliance: Collective Benefit Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data. Authority to control Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Responsibility Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Ethics Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem. These principles are presented as applying particularly to Indigenous data, but we believe t","date":"08 Feb 2022","objectID":"/fair-and-care/:0:0","tags":["FAIR","CARE"],"title":"What are the FAIR and CARE principles and why should corpus linguists know about them?","uri":"/fair-and-care/"}]