[{"categories":null,"content":"The ATAP workbench will enable researchers to work in ways which improve the reproducibility and accountability of their research. To do this, the output of your work in ATAP will be a fully documented research object. Graphic? Research objects are a single package that collects the data, code and other resources used in a piece of research and allows it to be a cite-able research output in its own right. At the end of your work in ATAP, there will be the automated option to package together the components of your workflow, such as: Raw data Transformed data A record of the notebooks which you have used Additional scripts and codes Results Visualisations High quality metadata ATAP will output an RO-Crate that contains these objects. An output of this kind can be assigned a unique identifier (such as a doi), and it can be published via services such as Zenodo or figshare so it can be cited in publications. This will make it easy to fulfil the increasingly common requirement of journals to make available the data that supports a publication. ","date":"15 Feb 2022","objectID":"/ATAP-website/research_objects/:0:0","tags":null,"title":"Research Objects","uri":"/ATAP-website/research_objects/"},{"categories":null,"content":"A little intro to the text analysis methods Data Preparation Useful Methods Research Objects ","date":"15 Feb 2022","objectID":"/ATAP-website/text_analysis/:0:0","tags":null,"title":"Text Analysis Overview","uri":"/ATAP-website/text_analysis/"},{"categories":null,"content":"Understanding Text as Data The term word is problematic. It is well-known to linguists that phonological words (defined by sound patterns), syntactic words (defined by combinatorial possibilities) and orthographic words (defined by the conventions of a writing system) do not always coincide. And even when we are only looking at written material, there are problems. How many words are there in this sentence? The cat sat on the mat One answer is that there are six words; that is, there are six groups of characters which are separated according to typographical convention. But there is another answer. There are five words, that is five distinct sequences of characters and one of those sequences (the) occurs twice. The terms standardly used to make this distinction are type and token. Tokens are instances of types, therefore if we count tokens, we count without considering repetition, while if we count types, we do consider repetition. There is a further distinction we may need to make which we can see if we consider another question: Are cat and cats the same word? They are distinct types, and therefore must also be distinct as tokens. But we have an intuition that at some level they are related, that there is some more abstract item which underlies both of them. This concept is usually referred to as a lemma (there is more about this concept on the Data Cleaning page). ","date":"15 Feb 2022","objectID":"/ATAP-website/text_analysis/:0:1","tags":null,"title":"Text Analysis Overview","uri":"/ATAP-website/text_analysis/"},{"categories":null,"content":"We need some sort of blurb in here. ","date":"15 Feb 2022","objectID":"/ATAP-website/organisation/:0:0","tags":null,"title":"Organisation","uri":"/ATAP-website/organisation/"},{"categories":null,"content":"Partner Institutions: University of Queensland: Professor Michael Haugh University of Sydney: Professor Monika Bednarek (Sydney Corpus Lab) Dr Joel Nothman (Sydney Informatics Hub) AARNet Dr Sara King Ryan Fraser ","date":"15 Feb 2022","objectID":"/ATAP-website/organisation/:1:0","tags":null,"title":"Organisation","uri":"/ATAP-website/organisation/"},{"categories":null,"content":"Project Team ","date":"15 Feb 2022","objectID":"/ATAP-website/organisation/:2:0","tags":null,"title":"Organisation","uri":"/ATAP-website/organisation/"},{"categories":null,"content":"Technology and Interoperability Team Lead: Peter Sefton Moises Sacal Marco La Rosa Michael D’Silva (AARNet) River Smith ","date":"15 Feb 2022","objectID":"/ATAP-website/organisation/:2:1","tags":null,"title":"Organisation","uri":"/ATAP-website/organisation/"},{"categories":null,"content":"Applications and Training Team Lead: Ben Foley Joel Nothman (Sydney Informatics Hub) Marius Mathers (Sydney Informatics Hub) ","date":"15 Feb 2022","objectID":"/ATAP-website/organisation/:2:2","tags":null,"title":"Organisation","uri":"/ATAP-website/organisation/"},{"categories":null,"content":"Data and Policy Team Lead: Kathrin Kaiser Cale Johnstone ","date":"15 Feb 2022","objectID":"/ATAP-website/organisation/:2:3","tags":null,"title":"Organisation","uri":"/ATAP-website/organisation/"},{"categories":null,"content":"Engagement and Outreach Team Lead: Simon Musgrave Sara King (AARNet) Leah Gustafson Harriet Shepard ","date":"15 Feb 2022","objectID":"/ATAP-website/organisation/:2:4","tags":null,"title":"Organisation","uri":"/ATAP-website/organisation/"},{"categories":null,"content":"Project Manager: Marco Fahmi ","date":"15 Feb 2022","objectID":"/ATAP-website/organisation/:2:5","tags":null,"title":"Organisation","uri":"/ATAP-website/organisation/"},{"categories":null,"content":"Project Coordinator: Leah Gustafson ","date":"15 Feb 2022","objectID":"/ATAP-website/organisation/:2:6","tags":null,"title":"Organisation","uri":"/ATAP-website/organisation/"},{"categories":null,"content":" Language Technology and Data Analysis Laboratory (LADAL) Short blurb about LADAL Common Spaces – Text Analytics with Jupyter Notebooks Short blurb about Common Spaces Common Language Resources and Technology Infrastructure (CLARIN) Short blurb about Clarin ","date":"15 Feb 2022","objectID":"/ATAP-website/resources/:0:0","tags":null,"title":"Resources","uri":"/ATAP-website/resources/"},{"categories":null,"content":"Introducing data preparation concepts This is a nice article about data wrangling from Harvard business school. They are using data wrangling as a high level term that refers to a whole range of data preparation methods. (https://online.hbs.edu/blog/post/data-wrangling). Not sure if the data wrangling section on the website was using the term as a specific methodology? This graphic is, sadly, all too true. Data scientists and those who are using data as part of their research spend much of their time preparing their dataset and transforming its structure into a format that can be used (often referred to as data wrangling or data munging). The Australian Text Analytics Platform will offer a range of tools to assist in cleaning text data and performing other preliminary operations which can prepare the data for analysis. ATAP analysis notebooks assume a common data structure, however the platform will provide notebooks containing code for transforming data into the structure that is needed for the procedure(s) in the analysis notebooks. There are two main processes that are needed to prepare text data for analysis and the ones that you will need to use will depend on the dataset that you are using. Those processes are: Cleaning Annotation ","date":"15 Feb 2022","objectID":"/ATAP-website/data_prep/:0:1","tags":null,"title":"Text Data Preparation","uri":"/ATAP-website/data_prep/"},{"categories":null,"content":"Common Cleaning Techniques Making all of the text lower case: This ensures that e.g. dog and Dog will not be treated as different items and is important if you are going to use analytic methods which rely on counting items. However, if you are planning to extract entities from your text data, retaining capital letters may be important. Standardising spelling: At least for English text, there are some well-known spelling variations, some with a geographical context (colour/color) and some that are more a matter of personal preference (recognise/recognize). As with case, standardising spelling ensures that pairs like the examples are treated as tokens of the same type. Removing stopwords: Stopwords are function words that are not interesting for analysis and we can safely remove them from our data using a stoplist. The 20 most frequently occurring stopwords in the British National Corpus are the, of, and, a, in, to, it, is, was, to, i, for, you, he, be, with, on, that, by, and at. The equivalent list for the Corpus of Contemporary American English (COCA) is the, be, and, of, a, in, to, have, to, it, I that, for, you, he, with, on, do, say and this. Some of the differences between the two are because the COCA counts are of lemmas which are the base forms of a word (think dog and dogs). Packages such as nltk and spaCy include standard stoplists for various languages, and it is possible to specify other words to be excluded. Removing punctuation: Punctuation can change how a text analysis package identifies a word. For instance to be sure that dogs and dog’s are not treated as different items, removing punctuation is good practice. Removing numbers: Sometimes the presence of numbers in documents can lead to artefacts in analysis. For example, in a collection of documents with page numbering, the numbers might show up as collocates of words or as part of a topic in a topic model. To avoid this, removing numbers is also good practice. This can present a challenge where numbers might be of interest (e.g. a study of mathematics textbooks). Removing whitespace: Whitespace can be another possible source of artefacts in analysis, especially if the source material uses a lot of tabs. is tabulated may be clearer? ","date":"15 Feb 2022","objectID":"/ATAP-website/data_prep/:0:2","tags":null,"title":"Text Data Preparation","uri":"/ATAP-website/data_prep/"},{"categories":null,"content":"Annotation Annotation is the process of adding information to your base dataset in order to make it possible to apply analytic techniques. In some cases, this may be a manual process. For example, much of the annotation which is described in the Text Encoding Initiative Guidelines requires a human making decisions although in some cases, manual annotation processes may also be scaled up to large text corpora using text classification or information extraction technologies. However some annotation can be carried out automatically, and there are two important kinds of annotation for text which fall into this category. Part-of-speech tagging (POS-tagging): For some analytic procedures, knowing which part of speech (or class of words) that an item belongs to is important. For languages where good analysis models exist, this annotation can be carried out automatically to a high level of accuracy – for English, we expect an accuracy rate better than 95%. POS-taggers generally provide more information than just whether an item is a noun or a verb, they also distinguish singular and plural forms of nouns and tell us whether a verb’s form is present tense form or a past tense. The tag sets which are used can therefore be quite extensive, and there are various tag sets in use such as the Penn Treebank tags and the CLAWS (British National Corpus) tags.  LADAL has some excellent resources that discuss POS tagging in more detail. Lemmatization: The distinctions between different forms of a single lexeme can be a hindrance in analysis especially if we are interested in lexical semantics in texts. Lemmatization identifies the base forms of words (or lemmas) in a text so that all forms of an item are treated together. For example: - dog and dogs will both be instances of the lemma DOG. - eat, eats, eating and ate will all be treated as tokens of the lemma EAT. As noted above, POS-tags give information about the form of words and are generally part of the annotation in lemmatization. A lemma, along with a POS-tag, can be reconstructed to the original form if necessary. ","date":"15 Feb 2022","objectID":"/ATAP-website/data_prep/:0:3","tags":null,"title":"Text Data Preparation","uri":"/ATAP-website/data_prep/"},{"categories":null,"content":"Introduction Throughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory LADAL at the University of Queensland. We also have given references to published research using the methods we discuss. LADAL has an overview of text analysis and distant reading. (context?) Counting Words More complex methods – classification More complex methods – others Visualisation Counting words When we count words, what are we trying to discover? Examples? ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:1","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Word frequency Tracking changes in the frequency of use of words across time has become popular since Google’s n-gram viewer has been available. However, results from this tool have to treated with caution for reasons set out in this blog-post. Expand/summarise the reasons/can this technique be used for other things? Comparing patterns of word frequency across texts can be part of authorship attribution. Patrick Juola describes using this method when he tried to decide whether Robert Galbraith was really J.K Rowling. This paper uses frequency and concordance analysis, with Australian data: Bednarek, Monika. 2020. Invisible or high-risk: Computer-assisted discourse analysis of references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes. PLoS ONE 15/6: e0234486. https://doi.org/10.1371/journal.pone.0234486 The ratio of types and tokens in a text has been used as a measure of lexical diversity in developmental and clinical studies as well as in stylistics. It has also been applied to theoretical problems in linguistics: Kettunen, Kimmo. 2014. Can Type-Token Ratio be Used to Show Morphological Complexity of Languages? Journal of Quantitative Linguistics 21(3). 223–245. https://doi.org/10.1080/09296174.2014.911506. ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:2","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Concordance A concordance allows the researcher to see all instances of a word or phrase in a text, neatly aligned in a column and with preceding and following context (see image below). Concordances are often a first step in analysis. The concordance allows a researcher to see how a word is used and in what contexts. Most concordancing tools allow sorting of results by either preceding or following words – the coloured text in the example below shows that in this case the results have been sorted hierarchically on the three following words. This possibility can help in discovering patterns of co-occurrence. Concordances are also very useful when looking for good examples to illustrate a point. (The type of display seen in the example is often referred to as KeyWord In Context – KWIC. There is a possibility of confusion here, as there is a separate analytic method commonly referred to as Keywords. (The example here was produced by Antconc.) check image for clarity This tutorial from LADAL on concordancing uses a notebook containing R code as a method of extracting concordance data. ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:3","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Clusters and collocations Two methods can be used for counting the co-occurrence of items in text. Clusters (sometimes known as n-grams) are sequences of adjacent items. A bigram is a sequence of two items, a trigram (3-gram) is a sequence of three items and so on. n-grams are types made up of more than one item, and therefore we can count the number of tokens of each n-gram in texts. n-grams are also the basis for a class of language models. (Google created a very large data set of English n-grams in developing their language-based algorithms and this data is available.) Example of n-grams? Collocations are patterns of co-occurrence in which the items are not necessarily adjacent. This is important because of the structure and use of verbs and their objects in English. The object of a verb is a noun phrase and in many cases the first item in an English noun phrase is a determiner. This means that for many transitive verbs, the bigram *verb the will occur quite frequently. But it is much more interesting to know whether there are patterns relating verbs and the entities which are their objects. Collocation analysis uncovers such patterns by looking at co-occurrences within a window of a certain size, for example three tokens on either side of the target. Collocation analysis gives information about the frequency of the co-occurrence of words and also a statistical measure of how likely that frequency is, given the overall frequencies of the terms in the corpus. Measures commonly applied include Mutual Information scores and Log-Likelihood scores. Collocations can also tell us about the meanings of words. If a word has collocates which fall into semantically distinct groups, this can indicate ambiguity or polysemy. And if different words share patterns of collocation, this can be evidence that the words are at least partial synonyms. This graphic (potentially too complicated: either an explanation needed or a simpler example) shows collocation relations in Darwin’s Origin of Species visualised as a network: This article uses bigram frequencies as part of an analysis of language change: Schweinberger, Martin. 2021. Ongoing change in the Australian English amplifier system. Australian Journal of Linguistics 41(2). 166–194. https://doi.org/10.1080/07268602.2021.1931028. An article which uses concordances and collocation analysis: Baker, Paul \u0026 Tony McEnery. 2005. A corpus-based approach to discourses of refugees and asylum seekers in UN and newspaper texts. Journal of Language and Politics 4(2). 197–226. This article uses the discovery of shared patterns of collocation as evidence that the words are at least partial synonyms: McEnery, Tony \u0026 Helen Baker. 2017. Corpus linguistics and 17th-century prostitution: computational linguistics and history (Corpus and Discourse. Research in Corpus and Discourse). London ; New York, NY: Bloomsbury Academic. (especially chapters 4 and 5) This tutorial from LADAL on analysing co-occurences and collocations uses a notebook containing R code as a method to extract and visualise semantic links between words. ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:4","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Keywords Keyword analysis is a statistically robust method of comparing frequencies of words in corpora. It tells us which words are more frequent (or less frequent) than would be expected in one text compared to another text and gives an estimate of the probability of the result. Keyword analysis uses two corpora: a target corpus, which is the material of interest, and a comparison corpus. Frequency lists are made for each corpus and then frequency of individual types in each corpus are compared. Keywords are ones which occur more frequently in the target corpus than expected given the reference corpus. The keyness of individual items is a quantitative measure of how unexpected the frequency is; chi-square is a one possible measure of this, but a log-likelihood measure is more commonly used. Positive keywords are …; negative keywords are words which occur less commonly than expected. A comparison of distinguishing words for three texts (Charles Darwin’s Origin, Herman Melville’s Moby Dick, and George Orwell’s 1984): What is this an example of? Keyness? Figure needs to be checked for clarity This paper applies keyword analysis to Australian text data sourced from a television series script: Bednarek, Monika. 2020. Keyword analysis and the indexing of Aboriginal and Torres Strait Islander identity: A corpus linguistic analysis of the Australian Indigenous TV drama Redfern Now. International Journal of Corpus Linguistics 25/4: 369-99. http://doi.org/10.1075/ijcl.00031.bed Tony McEnery describes using the keyword analysis method to compare four varieties of English in this chapter: McEnery, Tony. 2016. Keywords. In Paul Baker \u0026 Jesse Egbert (eds.), Triangulating methodological approaches in corpus-linguistic research (Routledge Advances in Corpus Linguistics 17), 20–32. New York: Routledge. This article explores how to assess Shakespeare’s use of words to build characters by using keyword analysis of the character’s dialog: Culpeper, Jonathan. 2002. Computers, language and characterisation: An analysis of six characters in Romeo and Juliet. In Conversation in Life and in Literature: Papers from the ASLA Symposium (Association Suedoise de Linguistique Appliquee (ASLA) 15), 11–30. Uppsala: Universitetstryckeriet. (pdf) ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:5","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"More complex methods – Classification Classification methods aim to assign some unit of analysis to a class. For example, a document (or a portion of a document) can be classified as having positive or negative sentiment. These methods are all examples of supervised machine learning. An algorithm is trained on the basis of annotated data to identify classifiers in the data – features which correlate in some way with the annotated classifications. If the algorithm achieves good results on testing data (classified by human judgment), then it can be used to classify unannotated data. ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:6","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Document Classification The task here is to assign documents to categories automatically. What is this used for in the research context? An common example of this procedure is spam filtering of email that may be applied by internet service providers and also within email applications. The following two articles taken together give an account of a technique for partially automating the training phase of classification and then of how the classifiers allowed researchers to access new information in a large and complex text. Leavy, Susan, Mark T Keane \u0026 Emilie Pine. 2019. Patterns in language: Text analysis of government reports on the Irish industrial school system with word embedding. Digital Scholarship in the Humanities 34(Supplement_1). i110–i122. https://doi.org/10.1093/llc/fqz012. Pine, Emilie, Susan Leavy \u0026 Mark T. Keane. 2017. Re-reading the Ryan Report: Witnessing via and Close and Distant Reading. Éire-Ireland 52(1–2). 198–215. https://doi.org/10.1353/eir.2017.0009. (available online) [Wikipedia] ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:7","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Sentiment analysis Sentiment analysis assigns (assigned to what? Could “sorts” be better?) documents according to the affect which they express. In simple cases, this can mean dividing (sorting?) documents into those which a express a positive view and those which express a negative view (with a neutral position sometimes also included). Those classifications are the basis for aggregated ratings - for example, online listings of movies and restaurants). A sentiment value is assigned to individual reviews then an aggregate score is calculated based on those values. The score then becomes … More sophisticated sentiment analysis can assign values on a scale. Some sentiment analysis tools use dictionaries of terms with sentiment values assigned to those terms; these are known as pre-trained or pre-determined classifiers. The following figure shows the results of the sentiment analysis of four texts (The Adventures of Huckleberry Finn by Mark Twain, 1984 by George Orwell, The Colour out of Space by H.P.Lovecraft, and On the Origin of Species by Charles Darwin) using the Word-Emotion Association Lexicon (Mohammad and Turney 2013): (check graphic for clarity) The Wikipedia entry for Sentiment Analysis gives more information and examples particularly in relation to the use of sentiment analysis as a tool used in online settings. LADAL’s Sentiment Analysis tutorial uses a notebook containing R code as a method of performing sentiment analysis. This article discusses problems in assembling training data for complex sentiment analysis tasks and then applies the results to oral history interviews with Holocaust survivors: Blanke, Tobias, Michael Bryant \u0026 Mark Hedges. 2020. Understanding memories of the Holocaust—A new approach to neural networks in the digital humanities. Digital Scholarship in the Humanities 35(1). 17–33. https://doi.org/10.1093/llc/fqy082. ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:8","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Named Entity Recognition Named Entity Recognition involves two levels of classification. First, segments of text are classified as either denoting or not denoting an entity: a person, a place or an organization. These entities can then be classified as belonging to a particular type. The Wikipedia entry explaining named-entity recognition gives further detail about the technique. This article looks at the problems encountered when applying a well-known entity recognition package (Stanford) to historical newspapers in the National Library of Australia’s Trove collection: Mac Kim, Sunghwan \u0026 Steve Cassidy. 2015. Finding names in trove: named entity recognition for Australian historical newspapers. In Proceedings of the Australasian Language Technology Association Workshop 2015, 57–65. (pdf) This article (section 6.3) discusses why entity recognition is not as useful as might be expected when studying names in novels: [title is kind of confusing with the next section being call computational stylistics also] Dalen-Oskam, K. van. 2013. Names in novels: An experiment in computational stylistics. Literary and Linguistic Computing 28(2). 359–370. https://doi.org/10.1093/llc/fqs007. ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:9","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Computational Stylistics (Stylometry) This is used to evaluate and analyse an author’s writing style by identifying their unique patterns of language use. It is also referred to as authorship attribution. [Why is this important?] The classification task is deciding whether a piece of text can be attributed to a particular author (and with what degree of confidence) [how? because? Based on what?]. Seemingly simple classifiers [such as?] are used for this task as they are assumed to be less open to conscious manipulation by writers. For example, comparative patterns of occurrence of function words [example] are considered a better classifier than occurrences of content words. Character n-grams have also proven to be a good classifier for use in this task. The Wikipedia entry on stylometry gives further information on the methodology. A classic stylometric study using Bayesian statistics rather than machine learning is: Mosteller, Frederick \u0026 David Lee Wallace. 1984. Applied Bayesian and classical inference: the case of the Federalist papers. New York: Springer-Verlag. This article applies stylometric techniques to a classic of Chinese literature: Zhu, Haoran, Lei Lei \u0026 Hugh Craig. 2021. Prose, Verse and Authorship in Dream of the Red Chamber : A Stylometric Analysis. Journal of Quantitative Linguistics 28(4). 289–305. https://doi.org/10.1080/09296174.2020.1724677. An overview of the use of function words in stylometry: Garcia, A. M. \u0026 J. C. Martin. 2007. Function Words in Authorship Attribution Studies. Literary and Linguistic Computing 22(1). 49–66. https://doi.org/10.1093/llc/fql048. ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:10","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"More complex methods – Others Topic models Topic modeling is a method which tries to recover [discover?] abstract ‘topics’ which occur in a collection of documents. The underlying theory is that different topics will tend to be associated with different words, and that different documents will tend to be associated with different topics. The complete model includes [outputs?] the strength of association (or probability) between each word and each topic, and between each topic and each document. A topic consists of a group of words and it is up to the researcher to decide if a semantically coherent interpretation can be given to any [identified?] topic. The below example visualisation is based on topic modeling of the State of the Union addresses given by US presidents, and shows the relative importance of different topics over time: The Wikipedia entry for topic models gives a more detailed explanation of the process. This topic modeling tutorial from LADAL uses R coding to process textual data and generate a topic model from that data. Poetics 41(6) is a journal issue devoted to the use of topic models in literary studies: the introduction to the journal (by Mohr and Bogdanov) provides a useful overview of the method. And this paper uses topic modeling as one tool in trying to improve access to a huge collection of scholarly literature: Mimno, David. 2012. Computational historiography: Data mining in a century of classics journals. Journal on Computing and Cultural Heritage 5(1). 1–19. https://doi.org/10.1145/2160165.2160168. ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:11","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Network Analysis Network analysis allows us to produce visualisations of the relationships between entities within a dataset. Analysis of social networks is a classic application of the method, but words and documents can also be thought of as entities and the relationships between them can then be analysed with this method. (see example visualisation of Darwin’s Origin of Species above) Here is another example of a network graph illustrating the relationships between the characters of Shakespeare’s Romeo and Juliet: [check graphic for clarity] This article gives several examples of how representing collocational link between words as a network can lead to insight into meaning relations: Brezina, Vaclav, Tony McEnery \u0026 Stephen Wattam. 2015. Collocations in context: A new perspective on collocation networks. International Journal of Corpus Linguistics 20(2). 139–173. https://doi.org/10.1075/ijcl.20.2.01bre. (pdf) Wikipedia has several good articles on network theory in general and social network analysis in particular. LADAL’s tutorial on Network Analysis introduces this method using R coding. ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:12","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Visualisation Visualisation is an important technique for exploring data, allowing us to see patterns easily, and also for presenting results. There are many methods for creating visualisations and this article gives an overview of some possibilities for visualising corpus data: Siirtola, Harri, Terttu Nevalainen, Tanja Säily \u0026 Kari-Jouko Räihä. 2011. Visualisation of text corpora: A case study of the PCEEC. How to Deal with Data: Problems and Approaches to the Investigation of the English Language over Time and Space. Helsinki: VARIENG 7. [html] If you want to see something more complex, this article includes animations showing change in use of semantic space over time – but you need to have full access to the online publication to see it. [Should we be sending people to things that they may not be able to access?] Hilpert, Martin \u0026 Florent Perek. 2015. Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts. Linguistics Vanguard 1(1). https://doi.org/10.1515/lingvan-2015-0013. This LADAL tutorial on data visualisation in R makes use of the ggplot package to create some common data visualisation using code. ","date":"15 Feb 2022","objectID":"/ATAP-website/methods/:0:13","tags":null,"title":"Useful Methods","uri":"/ATAP-website/methods/"},{"categories":null,"content":"Written by Simon Musgrave Data is becoming increasingly important in today’s world, so corpus linguists might feel that the rest of the world is finally catching up. But the rest of the world are bringing with them new approaches to how data is handled. This means that fields such as corpus linguistics may need to reassess their practices. Such reassessment includes addressing concerns about how data is stored and who can access it (data stewardship) – concerns that area part of the Open Science movement, ultimately grounded on principles of equity and accountability. The most influential approach to data stewardship today is the FAIR principles. According to these principles, data should be: - *Findable*   Metadata and data should be easy to find for both humans and computers. - *Accessible*   Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation. - *Interoperable*   The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing. - *Reusable*   The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings. In general, corpus linguists do well on the interoperability criterion. Corpus data is usually stored in non-proprietary formats; even when some structure is imposed on the data, this is almost always in a form which is saved as a simple text file (e.g. csv files or xml annotations). Data stored in such formats is easy to move between applications. But what about the other three criteria? Some corpus data is easy to discover; it is findable. For example CLARIN, the portal to the European Union language resource infrastructure, provides access to many large data collections, as does the Linguistic Data Consortium in the USA. However, some data is never made part of a large collection and often remains under the control of individual researchers or research teams. Such data may be almost impossible to find. Even if we can find such data, it is unlikely to be accompanied by good descriptions of the data and metadata, making reusability problematic. Of course, big corpora such as the British National Corpus will be both findable and accompanied by comprehensive corpus manuals. However, it is worth considering how to make other corpora more findable, including the provision of corpus manuals or corpus descriptions. Corpus resource databases such as the Corpus Resource Database (CoRD) do aim to work towards this principle. Accessibility may also be an issue for some data. Copyright law may allow use of material for individual research but prohibit any further distribution of the material. The FAIR approach to such cases is that metadata should be available so that interested parties can know that a data holding exists (F), and the metadata will include information about the conditions under which the data may or may not be shared or reused (A and R). (Image from Global Indigenous Data Alliance https://www.gida-global.org/) For linguists, there is another very important set of principles concerning data, the CARE principles developed by the Global Indigenous Data Alliance: Collective Benefit Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data. Authority to control Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Responsibility Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Ethics Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem. These principles are presented as appl","date":"08 Feb 2022","objectID":"/ATAP-website/moreblog/:0:0","tags":["new","blank"],"title":"Dummy Blog Post","uri":"/ATAP-website/moreblog/"},{"categories":null,"content":"Written by Simon Musgrave Data is becoming increasingly important in today’s world, so corpus linguists might feel that the rest of the world is finally catching up. But the rest of the world are bringing with them new approaches to how data is handled. This means that fields such as corpus linguistics may need to reassess their practices. Such reassessment includes addressing concerns about how data is stored and who can access it (data stewardship) – concerns that area part of the Open Science movement, ultimately grounded on principles of equity and accountability. The most influential approach to data stewardship today is the FAIR principles. According to these principles, data should be: - *Findable*   Metadata and data should be easy to find for both humans and computers. - *Accessible*   Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation. - *Interoperable*   The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing. - *Reusable*   The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings. In general, corpus linguists do well on the interoperability criterion. Corpus data is usually stored in non-proprietary formats; even when some structure is imposed on the data, this is almost always in a form which is saved as a simple text file (e.g. csv files or xml annotations). Data stored in such formats is easy to move between applications. But what about the other three criteria? Some corpus data is easy to discover; it is findable. For example CLARIN, the portal to the European Union language resource infrastructure, provides access to many large data collections, as does the Linguistic Data Consortium in the USA. However, some data is never made part of a large collection and often remains under the control of individual researchers or research teams. Such data may be almost impossible to find. Even if we can find such data, it is unlikely to be accompanied by good descriptions of the data and metadata, making reusability problematic. Of course, big corpora such as the British National Corpus will be both findable and accompanied by comprehensive corpus manuals. However, it is worth considering how to make other corpora more findable, including the provision of corpus manuals or corpus descriptions. Corpus resource databases such as the Corpus Resource Database (CoRD) do aim to work towards this principle. Accessibility may also be an issue for some data. Copyright law may allow use of material for individual research but prohibit any further distribution of the material. The FAIR approach to such cases is that metadata should be available so that interested parties can know that a data holding exists (F), and the metadata will include information about the conditions under which the data may or may not be shared or reused (A and R). (Image from Global Indigenous Data Alliance https://www.gida-global.org/) For linguists, there is another very important set of principles concerning data, the CARE principles developed by the Global Indigenous Data Alliance: Collective Benefit Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data. Authority to control Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Responsibility Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Ethics Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem. These principles are presented as appl","date":"08 Feb 2022","objectID":"/ATAP-website/fair-and-care/:0:0","tags":["FAIR","CARE"],"title":"What are the FAIR and CARE principles and why should corpus linguists know about them?","uri":"/ATAP-website/fair-and-care/"}]